# üöÄ Instala√ß√£o do Apache Airflow no WSL em modo Standalone

## üì¶ Requisitos
Instale o suporte a ambientes virtuais do Python:
- `sudo apt install -y python3-venv python3-pip`

## üìÇ Criar pasta do projeto
- `mkdir ~/airflow-standalone`
- `cd ~/airflow-standalone`

## üêç Criar e ativar ambiente virtual
- `python3 -m venv venv`
- `source venv/bin/activate`

## ‚¨ÜÔ∏è Atualizar o pip
- `python3 -m pip install --upgrade pip`

## ‚öôÔ∏è Instalar o Airflow
Defina a vers√£o desejada (exemplo: `3.1.0`) e use o arquivo de *constraints* para garantir compatibilidade das depend√™ncias:
- `AIRFLOW_VERSION=3.1.0`
- `PYTHON_VERSION="$(python3 --version | cut -d " " -f2 | cut -d. -f1-2)"`
- `CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"`
- `pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"`

üîé **Por que usar constraints?**  
O Airflow depende de muitos pacotes Python (Flask, SQLAlchemy, Pandas, etc.).  
Se voc√™ rodar apenas `pip install apache-airflow`, o `pip` pode instalar vers√µes mais novas desses pacotes que n√£o foram testadas e podem quebrar a compatibilidade.  
O arquivo de *constraints* garante que todas as depend√™ncias sejam instaladas em vers√µes validadas para aquela vers√£o do Airflow.  

Exemplo de um trecho de arquivo de constraints:  
- `Flask==2.2.5`  
- `SQLAlchemy==1.4.49`  
- `pendulum==2.1.2`  


Baixar as depend√™ncias no venv:

- `pip install pandas sqlalchemy numpy psycopg2-binary`

## ‚ñ∂Ô∏è Iniciar o Airflow
- `airflow standalone`

Esse comando ir√°:  
- Criar a pasta `~/airflow`  
- Inicializar o banco de dados SQLite  
- Criar um usu√°rio admin  
- Subir o webserver e o scheduler  

## üåê Acessar a interface web
Abra no navegador:  
- `http://localhost:8080`
üîë **Observa√ß√£o:**  
O **usu√°rio e senha** aparecem no terminal logo ap√≥s rodar `airflow standalone` ou no arquivo com o seguinte comando:

- `cat /home/geovany-cancio/airflow/simple_auth_manager_passwords.json.generated`

## üìÇ Criar pastas de DAGs e Tasks

- `cd ~/airflow`
- `mkdir -p ~/airflow/dags`
- `mkdir -p ~/airflow/plugins/custom_packages`

Configurar o airflow.cfg para n√£o aparecer dags de teste:
- `load_examples = False`

Ap√≥s a cria√ß√£o dos scripts copiar na pasta de dags e tasks:

- `cp /mnt/c/Users/geova/OneDrive/Documentos/empregadados/codigo/aulas_empregadados/aula_4_airflow/dags/new_pipeline_dag.py ~/airflow/dags/`
- `cp /mnt/c/Users/geova/OneDrive/Documentos/empregadados/codigo/aulas_empregadados/aula_4_airflow/custom_packages/plu_medical.py ~/airflow/plugins/custom_packages/`


# üöÄ Aula do sobre o Airflow (arquitetura e novos comandos)

1. Executar a instala√ß√£o do pacode do postgres.
   1. pip install 'apache-airflow-providers-postgres'
2. inserir no arquivo de configura√ß√£o o comando:
   1. [core] -> encontrar essa linha e abaixo inserir: template_searchpath = /home/geovany-cancio/airflow/plugins/custom_packages
3. Criar uma nova conex√£o na interface web para o postgresql.
4. Baixar o pacote para usar os sensores:
   1. pip install 'apache-airflow-providers-common-io'
   2. pip install apache-airflow-providers-filesystem
